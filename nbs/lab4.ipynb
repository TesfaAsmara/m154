{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ae6321",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"154 - Lab 4\"\n",
    "output: pdf_document\n",
    "date: \"2022-12-05\"\n",
    "---\n",
    "\n",
    "Here is my code for fitting a GPR.\n",
    "```{r}\n",
    "rbf.k <- function(x,y,gamma)\n",
    "  return(exp(-gamma*sum((x-y)^2)))\n",
    "\n",
    "true.f <- function(x) {\n",
    "  return(-(sin(x[1]*8) + x[2]^2 + x[1]*sqrt(x[2]) + 3 * dnorm(x[1],.5,.2) * dnorm(x[2],.5,.2)))  \n",
    "} \n",
    "\n",
    "\n",
    "gpreg <- function(x, y, gamma, sig, design) {\n",
    "  # Evaluates mean and covariance of GP at grid of points on [0,1]\n",
    "  # Inputs:\n",
    "  #   x, y: input and output values of data set\n",
    "  #   lam: smoothing parameter in RBF kernel\n",
    "  #   sig: error standard deviation of y\n",
    "  #   design: grid of points to evaluate the GP\n",
    "  # Returns: \n",
    "  #   mean=posterior mean, vars=posterior variance, and design=evaluation points\n",
    "  n <- length(y)\n",
    "  Sigma <- matrix(0,nrow=n+nrow(design), ncol=n+nrow(design))\n",
    "  all <- rbind(x, design)\n",
    "  for (i in 1:nrow(Sigma)) {\n",
    "    for (j in i:nrow(Sigma)) \n",
    "      Sigma[i,j] <- rbf.k(all[i,], all[j,], gamma) -> Sigma[j,i]\n",
    "  }\n",
    "  S11 <- Sigma[1:n, 1:n]\n",
    "  S12 <- Sigma[1:n, (n+1):ncol(Sigma)]\n",
    "  S21 <- Sigma[(n+1):ncol(Sigma), 1:n]\n",
    "  S22 <- Sigma[(n+1):ncol(Sigma),(n+1):ncol(Sigma)]  \n",
    "  inv <- S21%*%solve(S11+sig^2*diag(n))\n",
    "  mean <- inv%*%y\n",
    "  cov <- S22-inv%*%S12\n",
    "  vars <- diag(cov)\n",
    "  return(list(mean=mean, vars=vars))\n",
    "}\n",
    "```\n",
    "\n",
    "We are going to use Gaussian Process Optimization to try to minimize the function *true.f* above. \n",
    "\n",
    "We are going to start with 10 evaluations of the function, but at random points (for fun). Our grid is going to be fairly coarse, because there are some matrix operations required, and the code is not optimized.  \n",
    "```{r}\n",
    "\n",
    "###Sample Usage\n",
    "set.seed(4)\n",
    "n <- 10\n",
    "x <- cbind(runif(n),runif(n))\n",
    "y <- c()\n",
    "sig <- 0\n",
    "for (i in 1:n) \n",
    "  y[i] <- true.f(x[i,])\n",
    "y <- y + rnorm(n, 0, sig)\n",
    "\n",
    "design <- c()\n",
    "grid <- seq(0,1,.03)\n",
    "best.min <- 1e20\n",
    "truth <- matrix(0,nrow=length(grid), ncol=length(grid))\n",
    "for (i in 1:length(grid)) {\n",
    "  for (j in 1:length(grid)) {\n",
    "    design <- rbind(design, c(grid[i], grid[j]))\n",
    "    truth[i,j] <- true.f(c(grid[i], grid[j]))\n",
    "    if (truth[i,j] < best.min) {\n",
    "      best.min <- truth[i,j]\n",
    "      best.grid <- c(grid[i], grid[j])\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "gpfit <- gpreg(x, y, 10, 0.000, design)\n",
    "\n",
    "image(grid,grid,matrix(gpfit$mean,nrow=sqrt(length(gpfit$mean)), byrow = TRUE))\n",
    "points(x, col='blue',pch=19)\n",
    "image(grid,grid,truth)\n",
    "min(y)-min(truth)\n",
    "```\n",
    "\n",
    "Above are the estimated function based on the 10 evaluations (with the location of the evaluations visualized), the true function, and the available improvement.  Note that we need to choose a value of $\\gamma$ in fitting the GPR, where the kernel is $$K(x,y)=e^{-\\gamma(\\|x-y\\|^2)}$$\n",
    "So large $\\gamma$ means flexible learner.\n",
    "\n",
    "Let's compute the expected improvement (optimizing the possible improvement while not penalizing for evaluations that don't move the needle).  Then we'll do the additional evauliation, and refit the GPR.  It's possible to do fewer calculations in fitting the GRP (treating the old posterior as the new prior), but since the GRP fit is relatively inexpensive, we'll just refit the whole thing with the slightly larger sample size. \n",
    "```{r}\n",
    "f_prime <- min(y)\n",
    "ei <- c()\n",
    "for (i in 1:length(gpfit$mean)) {\n",
    "  if (gpfit$vars[i] > 0) {\n",
    "    ei[i] <- (f_prime - gpfit$mean[i])*pnorm(f_prime,gpfit$mean[i], sqrt(gpfit$vars[i])) + gpfit$vars[i]*dnorm(f_prime,gpfit$mean[i], sqrt(gpfit$vars[i]))\n",
    "  } else {\n",
    "    ei[i] <- 0\n",
    "  }\n",
    "}\n",
    "image(grid,grid,matrix(ei,nrow=sqrt(length(gpfit$mean)), byrow = TRUE))\n",
    "best.ei <- design[which.max(ei),]\n",
    "\n",
    "x <- rbind(x,best.ei)\n",
    "y <- c(y, true.f(best.ei))\n",
    "gpfit <- gpreg(x, y, 10, 0, design)\n",
    "\n",
    "image(grid,grid,matrix(gpfit$mean,nrow=sqrt(length(gpfit$mean)), byrow = TRUE))\n",
    "points(x, col='blue',pch=19)\n",
    "image(grid,grid,truth)\n",
    "min(y)-min(truth)\n",
    "\n",
    "```\n",
    "\n",
    "We can run the last chunck repeatedly, each time adding the current best new evaulation point.  It should find the bottom after 3 additional evaluations.\n",
    "\n",
    "Tune a learner with a bivariate tuning parameter (so randomForest with mtry and maxnodes or svm with cost and gamma) on a data set of your choosing using Gaussian Process Optimization. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m154] *",
   "language": "python",
   "name": "conda-env-m154-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
