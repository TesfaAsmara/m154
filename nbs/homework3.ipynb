{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "> Subject sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  In homework 2, we cross-validated a kernel density estimator by (probably) maximizing the leave-one-out likelihood/joint probability (maximize $\\prod_{i=1}^n \\hat{f}_{h,-i}(x_i)$ over $h$, yielding $h^*$).  \n",
    "\n",
    "a. Argue that it makes more sense to maximize the sum of the log density estimates instead (ref: lecture 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes more sense to maximize the sum of log density estimates instead because, by page 265 in The Elements of Statistical Learning, maximum likelihood is based on the likelihood function; we choose the value of $\\theta = \\hat{\\theta}$ that maximizes the loss $\\mathcal{L}$. However, this loss function is difficult to differentiate. Moreover, we know that the log function is a monotonically increasing function, meaning if the value on the x-axis increases, then the value on the y-axis also increases. Hence, the maximum value of the log of the likelihood occurs at the same point as the maximum value of the likelihood. Lastly, since the log of products is the sum of logs, we have simiplified the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Suppose that you will observe a second sample of the size $n$ as your future data.  How will the likelihood of this data ($\\prod_{j=n+1}^{2n} \\hat{f}_{h^*}(x_j))$) likely compare to the value of the cross validated leave-one-out likelihood? Why?  (You can check this with code easily, but it won't tell you why.  The answer is related to the reason we do cross-validation in the first place). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By page 265 in The Elements of Statistical Learning, leave-one-out cross-validation has low bias but can have high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Estimate the Air Quality at Estlla Hall (point 10 in the AQI data set) using a 2-$d$ kernel regression smoother based on data taken from purpleair.com.  If you use a \n",
    "\"circular\" bandwidth via a bivariate normal kernel, \n",
    "then the weights are just the product of the univariate kernel: if $w=(x,y)$ then $K_2(\\frac{\\|w_i-w\\|}{h})=K_1(\\frac{x_i-x}{h})K_1(\\frac{y_i-y}{h})$, where $K_2$ is the bivariate normal kernel and $K_1$ is a univariate normal kernel.  (If you've taken probability, state why this is true.  What is a \"circular\" bandwidth equivalent to?)    \n",
    "\n",
    "If you want to use something other than an ellipse with axis corresponding to the Lat/Lon axes, you can use the *dmvnorm* function in the **mvtnorm** package.  But I think the topography of this area lends itself nicely to the standard axes (though not necessarily circular neighborhoods).  \n",
    "\n",
    "Cross-validate only if you want to.  \n",
    "\n",
    "```{r}\n",
    "aqi <- read.csv(\"~/154/hw3aqidata\")  #you'll need to add this to your workspace\n",
    "stations <- aqi[1:9,]\n",
    "par(bg = \"blanchedalmond\")\n",
    "plot(c(0,2), c(-8,3), t='n')\n",
    "points(stations$x, stations$y, cex=4)\n",
    "for (i in 1:nrow(stations))\n",
    "  legend(stations$x[i]-.125, stations$y[i]+.6, stations$z[i], bty='n') #hack!\n",
    "points(aqi$x[10], aqi$y[10], cex=4)\n",
    "legend(aqi$x[10]-.155, aqi$y[10]+.6, c(' ?'), bty='n')\n",
    "```\n",
    "Note:  I got this data set as well as the XKCD data set using this tool: https://apps.automeris.io/wpd/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.282312</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.235627</td>\n",
       "      <td>-0.073905</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.230923</td>\n",
       "      <td>-1.036427</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.475306</td>\n",
       "      <td>-4.770928</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.618164</td>\n",
       "      <td>-4.770928</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1.280510</td>\n",
       "      <td>-4.144308</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1.458044</td>\n",
       "      <td>-4.282312</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>2.156367</td>\n",
       "      <td>-6.187040</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         x         y     z\n",
       "1      2  1.000000  2.282312  75.0\n",
       "2      3  0.235627 -0.073905  69.0\n",
       "3      4  0.230923 -1.036427  74.0\n",
       "4      5  0.475306 -4.770928  75.0\n",
       "5      6  0.618164 -4.770928  75.0\n",
       "6      7  1.280510 -4.144308  71.0\n",
       "7      8  1.458044 -4.282312  65.0\n",
       "8      9  2.156367 -6.187040  67.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_data = \"./data/hw3aqidata.csv\"\n",
    "aqi = pd.read_csv(path_to_data);\n",
    "stations = aqi[1:9]\n",
    "stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  The end of the first lab describes additive models and the back-fitting algorithm.  This model allows a high dimensional regression function to be estimated by recursively taking projections of your data to create bivariate data sets where the smoother from Lecture 2 can be used, thus avoiding the curse of dimensionality (at the cost of restricting the flexibility of your learner however).\n",
    "\n",
    "Implement this algorithm and fit the following data set ($x_1, x_2, y$) with the true response surface visualized. \n",
    "```{r}\n",
    "set.seed(47)\n",
    "x1 <- runif(200)\n",
    "x2 <- runif(200)\n",
    "y <- 5 + x1^2 + sin(x2*10) + rnorm(200,0, .3)\n",
    "grid <- seq(0,1,.01)\n",
    "truth <- matrix(0, nrow=length(grid), ncol=length(grid))\n",
    "for (i in 1:length(grid)){\n",
    "  for (j in 1:length(grid)) {\n",
    "    truth[i,j] <- 5 + grid[i]^2 + sin(grid[j]*10)\n",
    "  }\n",
    "}\n",
    "image(grid,grid,truth)  #heat map\n",
    "persp(truth, theta=45) #persepctive plot\n",
    "```\n",
    "You can either present a plot of the complete estimated regression surface, or plots of the individual estimates $\\hat{s}_i(x_i)$ which completely characterize the surface under the additivity assumption. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('m154env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "c6e244dd35c83ee1d23c25d30d19b3f743c34e80fa56282ef16d553800722480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
