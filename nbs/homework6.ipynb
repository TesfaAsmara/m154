{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b17ce3",
   "metadata": {},
   "source": [
    "1. Below is code to run a GPR. We continue to assume that $y_i=h(x_i) + \\epsilon_i$ and assume that the target $h$ is \"smooth\".\n",
    "\n",
    "Effectively, we write down the (prior) distribution of our target at the data inputs $h(x_i)$ and at a collection of grid points $h(g_i)$ as a multivariate normal/Gaussian, and compute the the posterior $$h(g_i)|h(x_i)=y_i - \\epsilon \\sim MVN(\\mu, \\Sigma),$$ which also happens to be a mutlivariate Gaussian (with fairly easy to compute mean vector $\\mu$ and covariance structure $\\Sigma$).  The normality and the covariance in the prior is the result of doing basis expansion, where each basis vector is $\\phi_i(x)=f_Z(\\frac{x-c_i}{\\lambda})$, where $f_Z(\\cdot)$ is the standard normal pdf and $c_i$ is the mean and $\\lambda$ is the standard deviation of a normal.  Infinite basis expansion with finite computation.  The **Kernel Trick**.\n",
    "\n",
    "There are at least two tuning parameters to deal with here, the spread of the normal basis functions, $\\lambda$, and the variance of the $\\epsilon$ errors, $\\sigma$.  We might learn these from a cross-validation routine, which gets called *Empirical Bayes*.\n",
    "```{r}\n",
    "rbf.k <- function(x,y,lambda)\n",
    "  return(exp(-1/(2*lambda^2)*(sum((x-y)^2))))\n",
    "\n",
    "target <- function(x) \n",
    "  return(log(x+.1)+sin(5*pi*x))\n",
    "\n",
    "gpreg <- function(x, y, lam, sig, design) {\n",
    "  \\# Evaluates mean and covariance of GP at grid of points on [0,1]\n",
    "  \\# Inputs:\n",
    "  \\#   x, y: input and output values of data set\n",
    "  \\#   lam: smoothing parameter in RBF kernel\n",
    "  \\#   sig: error standard deviation of y\n",
    "  \\#   design: grid of points to evaluate the GP\n",
    "  \\# Returns: \n",
    "  \\#   mean=posterior mean, vars=posterior variance, and design=evaluation points\n",
    "  n <- length(y)\n",
    "  x <- as.matrix(x)\n",
    "  design <- as.matrix(design)\n",
    "  m <- nrow(design)\n",
    "  Sigma <- matrix(0,nrow=n+m, ncol=n+m)\n",
    "  all <- rbind(x, design)\n",
    "  for (i in 1:nrow(Sigma)) {\n",
    "    for (j in i:nrow(Sigma)) \n",
    "      Sigma[i,j] <- rbf.k(all[i,], all[j,], lam) -> Sigma[j,i]\n",
    "  }\n",
    "  S11 <- Sigma[1:n, 1:n]\n",
    "  S12 <- Sigma[1:n, (n+1):ncol(Sigma)]\n",
    "  S21 <- Sigma[(n+1):ncol(Sigma), 1:n]\n",
    "  S22 <- Sigma[(n+1):ncol(Sigma),(n+1):ncol(Sigma)]  \n",
    "  inv <- S21%*%solve(S11+sig^2*diag(n))\n",
    "  mean <- inv%*%y\n",
    "  cov <- S22-inv%*%S12\n",
    "  vars <- diag(cov)\n",
    "  return(list(mean=mean, vars=vars))\n",
    "}\n",
    "\n",
    "\n",
    "\\#Sample Usage\n",
    "n <- 10\n",
    "x <- runif(n)\n",
    "y <- c()\n",
    "sig <- .1\n",
    "for (i in 1:n) y[i] <- target(x[i])\n",
    "y <- y + rnorm(n, 0, sig)\n",
    "\n",
    "design <- seq(0,1,.01)\n",
    "truth <- c()\n",
    "for (i in 1:length(design)) truth[i] <- target(design[i])\n",
    "\n",
    "gpfit <- gpreg(x, y, .1, sig, design)\n",
    "plot(c(0,1),c(min(gpfit$mean-2*sqrt(gpfit$vars)), max(gpfit$mean+2*sqrt(gpfit$vars))), \n",
    "  t=\"n\", xlab=\"x\", ylab=\"y\")\n",
    "points(x,y)\n",
    "lines(design, gpfit$mean)\n",
    "lines(design, gpfit$mean + 2*sqrt(gpfit$vars), col=\"blue\")\n",
    "lines(design, gpfit$mean - 2*sqrt(gpfit$vars), col=\"blue\")\n",
    "lines(design, truth, lty=3, col=\"red\")\n",
    "legend(0, 2, c(\"Truth\", \"Estimate\", \"Credible Bounds\"), c(\"black\", \"red\", \"blue\"))\n",
    "```\n",
    "\n",
    "We can also do this in higher dimesions. What shows up in the covariance is the squared Euclidian distance of the design points, so this works in any dimension (and the kernel trick means that we only need to compute the distance in the original $d$-dimensional data space, even though we are doing basis expansion. \n",
    "\n",
    "Here, we do it with a linear response surface, with the size of the points reflecting the value of the response variable. We need to be careful with the size of the grid, especially with my unoptimized code.  My grid is 25 by 25 and it still takes a few seconds to run. \n",
    "\n",
    "This one is overfit!\n",
    "```{r}\n",
    "a <- seq(0,1, length.out = 25)\n",
    "truth <- matrix(0, nrow=25, ncol=25)\n",
    "k <- 1\n",
    "grid <- matrix(0, nrow=25^2, ncol=2)\n",
    "for (i in 1:25) {\n",
    "  for (j in 1:25) {\n",
    "    truth[i,j] <- 3 + 5 * a[i] - 2 * a[j]\n",
    "    grid[k,] <- c(a[i],a[j])\n",
    "    k <- k + 1\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "x <- cbind(runif(50), runif(50))\n",
    "y <- 3 + 5 * x[,1] - 2 * x[,2] + rnorm(50,0,1)\n",
    "image(truth)\n",
    "points(x, cex=y)\n",
    "gpfit <- gpreg(x,y, .1, 1, grid)\n",
    "image(matrix(gpfit$mean, ncol=25, byrow=TRUE))\n",
    "points(x, cex=y)\n",
    "```\n",
    "\n",
    "Find a good smoothness level for both this problem here and revisit the response surface that we used the additive model on in a previous homework, this time fitting that data with a GPR. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f42def18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "from pymc3.distributions.timeseries import GaussianRandomWalk\n",
    "from scipy import optimize\n",
    "from theano import shared\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.uniform(0,1,(50, 2))\n",
    "y = 3 + 5 * X[:,0]**2 + np.sin(X[:,1])*10 + np.random.normal(0, np.sqrt(0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cdc30d",
   "metadata": {},
   "source": [
    "Let’s create a model with a shared parameter for specifying different levels of smoothing. We use very wide priors for the “mu” and “tau” parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19afc900",
   "metadata": {},
   "outputs": [],
   "source": [
    "LARGE_NUMBER = 1e5\n",
    "\n",
    "model = pm.Model()\n",
    "with model:\n",
    "    smoothing_param = shared(0.9)\n",
    "    mu = pm.Normal(\"mu\", sigma=LARGE_NUMBER)\n",
    "    tau = pm.Exponential(\"tau\", 1.0 / LARGE_NUMBER)\n",
    "    z = GaussianRandomWalk(\"z\", mu=mu, tau=tau / (1.0 - smoothing_param), shape=y.shape)\n",
    "    obs = pm.Normal(\"obs\", mu=z, tau=tau / smoothing_param, observed=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24b68ed",
   "metadata": {},
   "source": [
    "Let’s also make a helper function for inferring the most likely values of $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f7dff4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_z(smoothing):\n",
    "    with model:\n",
    "        smoothing_param.set_value(smoothing)\n",
    "        res = pm.find_MAP(vars=[z])\n",
    "        return res[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4b3e1280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00&lt;00:00 logp = -1.8141e+07, ||grad|| = 2,364.9]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 6.37342913  7.28873523  6.34630367  6.80060536  6.91575622  9.57418674\n",
      "  9.54674344  9.17459877 11.02474033 11.63640592  9.84853638 11.39743636\n",
      "  8.74249963  8.64534766  8.27629919  8.47924614  9.27264347  8.51622468\n",
      "  9.08332711 10.94984438 11.65906464 11.22302169 11.52752424 10.16231861\n",
      " 10.24959172 10.08296068 12.41904215 10.93286525 11.01706365 11.21768051\n",
      "  8.75088947  8.80541668  7.84820485  9.2676931  10.0800529   8.98637043\n",
      " 11.05686474 10.51137917  8.90559508  9.05338069  8.32992169  9.56533373\n",
      " 11.10514493  9.14252526  9.36579002  9.82545917  8.82458511 10.84647595\n",
      " 11.10009294 10.47216014]\n"
     ]
    }
   ],
   "source": [
    "smoothing = 0.5\n",
    "z_val = infer_z(smoothing)\n",
    "print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bc129",
   "metadata": {},
   "source": [
    "I will assume that \"the response surface that we used the additive model on in a previous homework\" refers to Homework 3, Question 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "55f8f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.uniform(0,1,(200,2))\n",
    "y = 5 + X[:,0]**2 + np.sin(X[:,1])*10 + np.random.normal(0, np.sqrt(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "effe6a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='15' class='' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [15/15 00:00&lt;00:00 logp = -1.8141e+07, ||grad|| = 2,364.9]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ 6.37342913  7.28873523  6.34630367  6.80060536  6.91575622  9.57418674\n",
      "  9.54674344  9.17459877 11.02474033 11.63640592  9.84853638 11.39743636\n",
      "  8.74249963  8.64534766  8.27629919  8.47924614  9.27264347  8.51622468\n",
      "  9.08332711 10.94984438 11.65906464 11.22302169 11.52752424 10.16231861\n",
      " 10.24959172 10.08296068 12.41904215 10.93286525 11.01706365 11.21768051\n",
      "  8.75088947  8.80541668  7.84820485  9.2676931  10.0800529   8.98637043\n",
      " 11.05686474 10.51137917  8.90559508  9.05338069  8.32992169  9.56533373\n",
      " 11.10514493  9.14252526  9.36579002  9.82545917  8.82458511 10.84647595\n",
      " 11.10009294 10.47216014]\n"
     ]
    }
   ],
   "source": [
    "smoothing = 0.5\n",
    "z_val = infer_z(smoothing)\n",
    "print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a7f531",
   "metadata": {},
   "source": [
    "2.  $k$-NN and the kernel trick.\n",
    "\n",
    "Recall (if you have a linear algebra background) the *dot product* between two vectors ${\\bf x}$ and ${\\bf y}$ is given as ${\\bf x}\\cdot\\bf{y}=\\sum_{i=1}^p x_iy_i$ and the squared norm is $\\|x\\|^2=x\\cdot x$\n",
    "\n",
    "a.  Show that the kernel function $$K(x,y)=x\\cdot y + ||x||^2||y||^2$$ for $x,y\\in\\mathbb{R}^2$ corresponds to augmenting the data space with a single extra feature, $x_3=x_1^2+x_2^2$, so that $\\phi: \\mathbb{R}^2\\to\\mathbb{R}^3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3970d",
   "metadata": {},
   "source": [
    "$K(x, y) = \\sum_{i=1}^p x_iy_i + (\\sqrt{x_1^2 + \\dots + x_n^2}^2)(\\sqrt{y_1^2 + \\dots y_n^2}^2) = \\sum_{i=1}^p x_iy_i +(x_1^2 + \\dots x_n^2)(y_1^2 + \\dots + y_n^2) = \\sum_{i=1}^p x_iy_i + x_1^2y_1^2+ \\dots x_1^2y_n^2 + \\dots x_n^2y_n^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbafe890",
   "metadata": {},
   "source": [
    "b. The $k$-NN classifier only relies on the Euclidian distance between points:  $$\\|{\\bf x}-{\\bf y}\\|^{1/2}=\\sqrt{\\sum_i (x_i-y_i)^2}$$ \n",
    "\n",
    "Show that this algorithm is **kernelizable**, i.e. the only way the algorithm uses the data is contained in the **Gram matrix**, the matrix of dot (inner) products ${\\bf G}:=[{\\bf x}_i\\cdot{\\bf x}_j]_{i,j=1}^n$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5e73a",
   "metadata": {},
   "source": [
    "By page 35 in ESL, the metric for the $k$-NN classifier is $K_k(x, x_0) = I(||x-x_0|| \\leq ||x_{(k)}-x_0||)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d2c4a",
   "metadata": {},
   "source": [
    "c.  Here's my function for $k$-NN, with a slightly different generation of the distance matrix (less elegant but easier to see what it's doing). \n",
    "```{r}\n",
    "dist.comp <- function(pt, data) {\n",
    "  dists <- c()\n",
    "  for (i in 1:nrow(data)) {\n",
    "    dists[i] <- sqrt(sum((pt-data[i,])^2))\n",
    "  }\n",
    "  return(dists)\n",
    "}\n",
    "\n",
    "knn <- function(pt, data, labels, k) {\n",
    "  \\# length of pt should match ncol of data\n",
    "  dists <- dist.comp(pt, data)\n",
    "  inds <- which(dists <= sort(dists)[k])\n",
    "  names(which.max(table(labels[inds])))\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "and it implemented on the iris data (note: this code is pretty slow!)\n",
    "\n",
    "```{r}\n",
    "pred.iris <- c()\n",
    "for (i in 1:150) {\n",
    "  pred.iris[i] <- knn(iris[i,1:4], iris[-i,1:4], iris[-i,5], 4)\n",
    "}\n",
    "pred.iris\n",
    "```\n",
    "\n",
    "Consider the following data set\n",
    "\n",
    "```{r}\n",
    "set.seed(47)\n",
    "r <- c(runif(75,0,.5), runif(75,.5,1))\n",
    "theta <- runif(150,0,2*pi)\n",
    "x <- r*cos(theta); y <- r*sin(theta)\n",
    "classes <- c('in', 'out')\n",
    "label <- classes[(r>.5)+1]\n",
    "plot(x,y, col=\"blue\")\n",
    "points(x[label==\"in\"], y[label==\"in\"], col='red')\n",
    "```\n",
    "\n",
    "Let's try $3$-NN on this data\n",
    "\n",
    "```{r}\n",
    "pred.bull <- c()\n",
    "for (i in 1:150) {\n",
    "  pred.bull[i] <- knn(c(x[i], y[i]), cbind(x[-i], y[-i]), label[-i], 3)\n",
    "}\n",
    "rbind(pred.bull, label)\n",
    "mean(pred.bull==label)  \\#true classification rate\n",
    "plot(x,y, col=\"blue\")\n",
    "points(x[label==\"in\"], y[label==\"in\"], col='red')\n",
    "points(x,y, cex=.5, col='blue')\n",
    "points(x[pred.bull==\"in\"], y[pred.bull==\"in\"], col='red', cex=.5)\n",
    "```\n",
    "Not bad,  but perfect is clearly attainable (given the data generation, the true decision boundary is the circle with radius .5)\n",
    "\n",
    "Edit the distance function, replacing the Euclidean distance with its representation in terms of dot products, and replace those dot products with the kernel function given above (it's the same as augmenting the data, but I want you to use the kernel).  Rerun this algorithm and discuss your findings.  \n",
    "\n",
    "Will this strategy always work?  Or is it particular to something about this data set? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532468c",
   "metadata": {},
   "source": [
    "I assume that this strategy will always work if the substitution is truly equivalent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m154] *",
   "language": "python",
   "name": "conda-env-m154-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
