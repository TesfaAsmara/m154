{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b17ce3",
   "metadata": {},
   "source": [
    "# 1. \n",
    "\n",
    "Below is code to run a GPR. We continue to assume that $y_i=h(x_i) + \\epsilon_i$ and assume that the target $h$ is \"smooth\".\n",
    "\n",
    "Effectively, we write down the (prior) distribution of our target at the data inputs $h(x_i)$ and at a collection of grid points $h(g_i)$ as a multivariate normal/Gaussian, and compute the the posterior $$h(g_i)|h(x_i)=y_i - \\epsilon \\sim MVN(\\mu, \\Sigma),$$ which also happens to be a mutlivariate Gaussian (with fairly easy to compute mean vector $\\mu$ and covariance structure $\\Sigma$).  The normality and the covariance in the prior is the result of doing basis expansion, where each basis vector is $\\phi_i(x)=f_Z(\\frac{x-c_i}{\\lambda})$, where $f_Z(\\cdot)$ is the standard normal pdf and $c_i$ is the mean and $\\lambda$ is the standard deviation of a normal.  Infinite basis expansion with finite computation.  The **Kernel Trick**.\n",
    "\n",
    "There are at least two tuning parameters to deal with here, the spread of the normal basis functions, $\\lambda$, and the variance of the $\\epsilon$ errors, $\\sigma$.  We might learn these from a cross-validation routine, which gets called *Empirical Bayes*.\n",
    "```{r}\n",
    "rbf.k <- function(x,y,lambda)\n",
    "  return(exp(-1/(2*lambda^2)*(sum((x-y)^2))))\n",
    "\n",
    "target <- function(x) \n",
    "  return(log(x+.1)+sin(5*pi*x))\n",
    "\n",
    "gpreg <- function(x, y, lam, sig, design) {\n",
    "  # Evaluates mean and covariance of GP at grid of points on [0,1]\n",
    "  # Inputs:\n",
    "  #   x, y: input and output values of data set\n",
    "  #   lam: smoothing parameter in RBF kernel\n",
    "  #   sig: error standard deviation of y\n",
    "  #   design: grid of points to evaluate the GP\n",
    "  # Returns: \n",
    "  #   mean=posterior mean, vars=posterior variance, and design=evaluation points\n",
    "  n <- length(y)\n",
    "  x <- as.matrix(x)\n",
    "  design <- as.matrix(design)\n",
    "  m <- nrow(design)\n",
    "  Sigma <- matrix(0,nrow=n+m, ncol=n+m)\n",
    "  all <- rbind(x, design)\n",
    "  for (i in 1:nrow(Sigma)) {\n",
    "    for (j in i:nrow(Sigma)) \n",
    "      Sigma[i,j] <- rbf.k(all[i,], all[j,], lam) -> Sigma[j,i]\n",
    "  }\n",
    "  S11 <- Sigma[1:n, 1:n]\n",
    "  S12 <- Sigma[1:n, (n+1):ncol(Sigma)]\n",
    "  S21 <- Sigma[(n+1):ncol(Sigma), 1:n]\n",
    "  S22 <- Sigma[(n+1):ncol(Sigma),(n+1):ncol(Sigma)]  \n",
    "  inv <- S21%*%solve(S11+sig^2*diag(n))\n",
    "  mean <- inv%*%y\n",
    "  cov <- S22-inv%*%S12\n",
    "  vars <- diag(cov)\n",
    "  return(list(mean=mean, vars=vars))\n",
    "}\n",
    "\n",
    "\n",
    "###Sample Usage\n",
    "n <- 10\n",
    "x <- runif(n)\n",
    "y <- c()\n",
    "sig <- .1\n",
    "for (i in 1:n) y[i] <- target(x[i])\n",
    "y <- y + rnorm(n, 0, sig)\n",
    "\n",
    "design <- seq(0,1,.01)\n",
    "truth <- c()\n",
    "for (i in 1:length(design)) truth[i] <- target(design[i])\n",
    "\n",
    "gpfit <- gpreg(x, y, .1, sig, design)\n",
    "plot(c(0,1),c(min(gpfit$mean-2*sqrt(gpfit$vars)), max(gpfit$mean+2*sqrt(gpfit$vars))), \n",
    "  t=\"n\", xlab=\"x\", ylab=\"y\")\n",
    "points(x,y)\n",
    "lines(design, gpfit$mean)\n",
    "lines(design, gpfit$mean + 2*sqrt(gpfit$vars), col=\"blue\")\n",
    "lines(design, gpfit$mean - 2*sqrt(gpfit$vars), col=\"blue\")\n",
    "lines(design, truth, lty=3, col=\"red\")\n",
    "legend(0, 2, c(\"Truth\", \"Estimate\", \"Credible Bounds\"), c(\"black\", \"red\", \"blue\"))\n",
    "```\n",
    "\n",
    "We can also do this in higher dimesions. What shows up in the covariance is the squared Euclidian distance of the design points, so this works in any dimension (and the kernel trick means that we only need to compute the distance in the original $d$-dimensional data space, even though we are doing basis expansion. \n",
    "\n",
    "Here, we do it with a linear response surface, with the size of the points reflecting the value of the response variable. We need to be careful with the size of the grid, especially with my unoptimized code.  My grid is 25 by 25 and it still takes a few seconds to run. \n",
    "\n",
    "This one is overfit!\n",
    "```{r}\n",
    "a <- seq(0,1, length.out = 25)\n",
    "truth <- matrix(0, nrow=25, ncol=25)\n",
    "k <- 1\n",
    "grid <- matrix(0, nrow=25^2, ncol=2)\n",
    "for (i in 1:25) {\n",
    "  for (j in 1:25) {\n",
    "    truth[i,j] <- 3 + 5 * a[i] - 2 * a[j]\n",
    "    grid[k,] <- c(a[i],a[j])\n",
    "    k <- k + 1\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "x <- cbind(runif(50), runif(50))\n",
    "y <- 3 + 5 * x[,1] - 2 * x[,2] + rnorm(50,0,1)\n",
    "image(truth)\n",
    "points(x, cex=y)\n",
    "gpfit <- gpreg(x,y, .1, 1, grid)\n",
    "image(matrix(gpfit$mean, ncol=25, byrow=TRUE))\n",
    "points(x, cex=y)\n",
    "```\n",
    "\n",
    "Find a good smoothness level for both this problem here and revisit the response surface that we used the additive model on in a previous homework, this time fitting that data with a GPR. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42def18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62a7f531",
   "metadata": {},
   "source": [
    "# 2.  $k$-NN and the kernel trick.\n",
    "\n",
    "Recall (if you have a linear algebra background) the *dot product* between two vectors ${\\bf x}$ and ${\\bf y}$ is given as ${\\bf x}\\cdot\\bf{y}=\\sum_{i=1}^p x_iy_i$ and the squared norm is $\\|x\\|^2=x\\cdot x$\n",
    "\n",
    "## a. \n",
    "\n",
    "Show that the kernel function\n",
    "$$K(x,y)=x\\cdot y + ||x||^2||y||^2$$\n",
    "for $x,y\\in\\mathbb{R}^2$ corresponds to augmenting the data space with a single extra feature, $x_3=x_1^2+x_2^2$, so that $\\phi: \\mathbb{R}^2\\to\\mathbb{R}^3$.\n",
    "\n",
    "## b. \n",
    "\n",
    "The $k$-NN classifier only relies on the Euclidian distance between points:  $$\\|{\\bf x}-{\\bf y}\\|^{1/2}=\\sqrt{\\sum_i (x_i-y_i)^2}$$ \n",
    "\n",
    "Show that this algorithm is **kernelizable**, i.e. the only way the algorithm uses the data is contained in the **Gram matrix**, the matrix of dot (inner) products ${\\bf G}:=[{\\bf x}_i\\cdot{\\bf x}_j]_{i,j=1}^n$\n",
    "\n",
    "## c. \n",
    "\n",
    "Here's my function for $k$-NN, with a slightly different generation of the distance matrix (less elegant but easier to see what it's doing). \n",
    "```{r}\n",
    "dist.comp <- function(pt, data) {\n",
    "  dists <- c()\n",
    "  for (i in 1:nrow(data)) {\n",
    "    dists[i] <- sqrt(sum((pt-data[i,])^2))\n",
    "  }\n",
    "  return(dists)\n",
    "}\n",
    "\n",
    "knn <- function(pt, data, labels, k) {\n",
    "  ## length of pt should match ncol of data\n",
    "  dists <- dist.comp(pt, data)\n",
    "  inds <- which(dists <= sort(dists)[k])\n",
    "  names(which.max(table(labels[inds])))\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "and it implemented on the iris data (note: this code is pretty slow!)\n",
    "\n",
    "```{r}\n",
    "pred.iris <- c()\n",
    "for (i in 1:150) {\n",
    "  pred.iris[i] <- knn(iris[i,1:4], iris[-i,1:4], iris[-i,5], 4)\n",
    "}\n",
    "pred.iris\n",
    "```\n",
    "\n",
    "Consider the following data set\n",
    "\n",
    "```{r}\n",
    "set.seed(47)\n",
    "r <- c(runif(75,0,.5), runif(75,.5,1))\n",
    "theta <- runif(150,0,2*pi)\n",
    "x <- r*cos(theta); y <- r*sin(theta)\n",
    "classes <- c('in', 'out')\n",
    "label <- classes[(r>.5)+1]\n",
    "plot(x,y, col=\"blue\")\n",
    "points(x[label==\"in\"], y[label==\"in\"], col='red')\n",
    "```\n",
    "\n",
    "Let's try $3$-NN on this data\n",
    "\n",
    "```{r}\n",
    "pred.bull <- c()\n",
    "for (i in 1:150) {\n",
    "  pred.bull[i] <- knn(c(x[i], y[i]), cbind(x[-i], y[-i]), label[-i], 3)\n",
    "}\n",
    "rbind(pred.bull, label)\n",
    "mean(pred.bull==label)  #true classification rate\n",
    "plot(x,y, col=\"blue\")\n",
    "points(x[label==\"in\"], y[label==\"in\"], col='red')\n",
    "points(x,y, cex=.5, col='blue')\n",
    "points(x[pred.bull==\"in\"], y[pred.bull==\"in\"], col='red', cex=.5)\n",
    "```\n",
    "Not bad,  but perfect is clearly attainable (given the data generation, the true decision boundary is the circle with radius .5)\n",
    "\n",
    "Edit the distance function, replacing the Euclidean distance with its representation in terms of dot products, and replace those dot products with the kernel function given above (it's the same as augmenting the data, but I want you to use the kernel).  Rerun this algorithm and discuss your findings.  \n",
    "\n",
    "Will this strategy always work?  Or is it particular to something about this data set? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:m154] *",
   "language": "python",
   "name": "conda-env-m154-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
